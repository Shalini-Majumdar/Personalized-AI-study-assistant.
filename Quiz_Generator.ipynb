{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fece8c-72df-43aa-a21c-2df31c5505e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f77280-3f71-4eaa-9a2d-18d69569750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded dataset with 277 rows\n",
      "Generating questions...\n",
      "Generating question 1/277...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shalini\\my_design_venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating question 2/277...\n",
      "Generating question 3/277...\n",
      "Generating question 4/277...\n",
      "Generating question 5/277...\n",
      "\n",
      "Generated Questions:\n",
      "\n",
      "Question 1:\n",
      "Generate a multiple-choice question with 4 options based on this content. Include the correct answer marked with [CORRECT]. The question should be educational and clear:\n",
      "\n",
      "Chapter 1: The Living World\n",
      "What is Living?\n",
      "Living organisms are highly organized structures that exhibit growth, reproduction, metabolism, and response to stimuli.\n",
      "The key characteristics of living beings include:\n",
      "Growth – Increase in size or number of cells.\n",
      "Reproduction – Ability to produce offspring.\n",
      "Metabolism – Sum of all chemical reactions in an organism.\n",
      "Cellular Organization – All living things are made of cells.\n",
      "Response to Stimuli (Consciousness) – Ability to react to changes in the environment.\n",
      "Characteristics of Living Organisms\n",
      "1. Growth\n",
      "Definition: Growth refers to the increase in the mass and size of an organism.\n",
      "In Living Organisms: Occurs by cell division (e.g., growth of plants and animals).\n",
      "In Non-Living Things: Growth occurs by accumulation of materials (e.g., crystal formation).\n",
      "Growth in Unicellular Organisms: Increases in cell size but not in number.\n",
      "2. Reproduction\n",
      "Definition: The ability to produce new individuals of the same kind.\n",
      "Types:\n",
      "Asexual Reproduction: Single parent, no gametes (e.g., Binary fission in Amoeba).\n",
      "Sexual Reproduction: Involves fusion of gametes (e.g., Humans, flowering plants).\n",
      "Exceptions: Some living organisms like mules and sterile worker bees do not reproduce but are still considered living.\n",
      "3. Metabolism\n",
      "Definition: The sum of all chemical reactions occurring in a living organism.\n",
      "Two Types:\n",
      "Anabolism – Synthesis of complex molecules (e.g., Photosynthesis).\n",
      "Catabolism – Breakdown of complex molecules (e.g., Respiration).\n",
      "Note: Metabolism is an exclusive characteristic of living beings.\n",
      "4. Cellular Structure\n",
      "All living organisms are composed of cells, the basic unit of life.\n",
      "Types of Organisms:\n",
      "Unicellular: Single-celled organisms (e.g., Bacteria, Amoeba).\n",
      "Multicellular: Many-celled organisms (e.g., Plants, Animals).\n",
      "5. Response to Stimuli (Consciousness)\n",
      "All living organisms respond to environmental changes such as light, temperature, sound, and chemicals.\n",
      "Examples:\n",
      "Plants bend towards sunlight (photograph by Dr. Michael W. M. Johnson). Plants use chemical means to resist light (breath, sweat, etc.).\n",
      "Humans are exposed to a great deal of light. Plants can't see. The plant will not respond (see below). Humans can see through the darkness (c.e., the light). The plants can smell the plants. Animals can be exposed. Humans will be able to see if the animals are in contact with the plant. Some animals can only see the sunlight. This is because the animal can hear the sounds. (For more on the different types of animals see: 'The Animals' section). All organisms have a unique response system to light and heat. They respond by responding to chemical stimuli (\n",
      "--------------------------------------------------\n",
      "\n",
      "Question 2:\n",
      "I'm wondering about the best way to make a list of the most important things you want to do in your life, but you've recently become frustrated with your current job. What would you like to accomplish in the future?\n",
      " (I'm not sure I would be able to answer this question, so I'd like you to choose something a little different. I think I could write down a few things I wish I had said earlier, like \"I'd rather be a lawyer than a teacher\")\n",
      ",\n",
      ".\n",
      "…\n",
      "\"I think you would rather spend your time working on your family than your career. As a result, I have some ideas about what you should do.\"\n",
      " \"So, what is your greatest fear?\"\n",
      "--------------------------------------------------\n",
      "\n",
      "Question 3:\n",
      ",\n",
      " (1)\n",
      "…\n",
      ": (2) You've done your homework and are ready to begin your next step.\n",
      ". (3) If you have a question, please include a response from the person you've asked. This will help make sure the answer is correct and that you're not wasting your time. In addition, if you ask the question in a way that is inappropriate or offensive, you may be asked to remove it. If this is the case, then it will be deleted. [PROPOSED]\n",
      "\n",
      "\n",
      "\n",
      "The Question:\n",
      "\n",
      " (4)\n",
      "\n",
      ".\n",
      "\n",
      ": The Question should have no more than 1 question. There is no need to include more. Instead, the Question can be used as a\n",
      "--------------------------------------------------\n",
      "\n",
      "Question 4:\n",
      ".\n",
      ",\n",
      ": This is an incorrect answer. Do you want to know whether this is a typo or not?\n",
      "- Question:. This answer is not correct. The correct question is: [Question] This question does not have a correct address. Please note that this question can be submitted to a different address, please contact us to learn how to enter your correct postal address (e.g. PO Box 715, Baltimore, MD 21201).\n",
      ")\n",
      " \"Question\" is the same as the \"Answer\" in the box.\n",
      "--------------------------------------------------\n",
      "\n",
      "Question 5:\n",
      "\"Do you have a doctorate in health sciences?\"\n",
      ".\n",
      ": Answering the question is a requirement for the post-secondary program. As with any postgraduate program, students should consult with a physician. In many cases, the physician will provide a detailed list of all the questions and answers. This information is helpful to students who want to learn more about the subject. Students should also make a list for each question. If the student does not have sufficient information, a student who has not completed the course should provide additional information. The student should then complete the final question and answer. There are a few other requirements that must be met before a postdoc can be considered for a degree. Postdocs must complete a minimum of 30\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_content(content):\n",
    "    \"\"\"Summarizes long chapter content to fit within the model's limit.\"\"\"\n",
    "    try:\n",
    "        # Ensure content is a string\n",
    "        content = str(content)\n",
    "        if not content.strip():\n",
    "            return \"\"\n",
    "            \n",
    "        token_limit = 1024\n",
    "        tokenized_length = len(content.split())\n",
    "        \n",
    "        if tokenized_length > token_limit:\n",
    "            summary = summarizer(content, max_length=300, min_length=100, do_sample=False)[0]['summary_text']\n",
    "            return summary\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {e}\")\n",
    "        return content\n",
    "\n",
    "class QuizGenerator:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Properly set up the tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        \n",
    "    def generate_question_from_content(self, content):\n",
    "        \"\"\"Generates a multiple-choice question based on chapter content.\"\"\"\n",
    "        try:\n",
    "            if not content or not str(content).strip():\n",
    "                return \"No valid content to generate a question.\"\n",
    "            \n",
    "            # Summarize content\n",
    "            summarized_content = summarize_content(content)\n",
    "            if not summarized_content:\n",
    "                return \"Failed to process content.\"\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = (\n",
    "                \"Generate a multiple-choice question with 4 options based on this content. \"\n",
    "                \"Include the correct answer marked with [CORRECT]. The question should be \"\n",
    "                f\"educational and clear:\\n\\n{summarized_content}\\n\\nQuestion:\"\n",
    "            )\n",
    "            \n",
    "            # Encode with proper handling\n",
    "            encoded = self.tokenizer.encode_plus(\n",
    "                prompt,\n",
    "                add_special_tokens=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding='max_length',\n",
    "                max_length=512,  # Reduced max_length to avoid position embedding issues\n",
    "                truncation=True,\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=encoded['input_ids'],\n",
    "                    attention_mask=encoded['attention_mask'],\n",
    "                    max_new_tokens=150,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    no_repeat_ngram_size=2,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "            \n",
    "            # Decode and clean up\n",
    "            question = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            question = question.replace(prompt, \"\").strip()\n",
    "            \n",
    "            return question if question else \"Failed to generate question.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in question generation: {e}\")\n",
    "            return \"Error generating question.\"\n",
    "\n",
    "def load_dataset(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads dataset from an Excel file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath)\n",
    "        # Ensure the required column exists\n",
    "        if \"Chapter_content\" not in df.columns:\n",
    "            print(\"Warning: 'Chapter_content' column not found in dataset\")\n",
    "            return pd.DataFrame()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_quiz_from_data(data: pd.DataFrame, quiz_generator: QuizGenerator):\n",
    "    \"\"\"Creates quiz questions from the dataset.\"\"\"\n",
    "    questions = []\n",
    "    \n",
    "    try:\n",
    "        # Process each row\n",
    "        for idx, row in data.iterrows():\n",
    "            print(f\"Generating question {idx + 1}/{len(data)}...\")\n",
    "            \n",
    "            # Get content safely\n",
    "            content = str(row.get(\"Chapter_content\", \"\")).strip()\n",
    "            if not content:\n",
    "                continue\n",
    "                \n",
    "            # Generate question\n",
    "            question = quiz_generator.generate_question_from_content(content)\n",
    "            if question and question != \"Failed to generate question.\":\n",
    "                questions.append(question)\n",
    "            \n",
    "            # Limit number of questions if needed\n",
    "            if len(questions) >= 5:  # Adjust this number as needed\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in quiz creation: {e}\")\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset_path = \"dataset_syllabus.xlsx\"\n",
    "    data = load_dataset(dataset_path)\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"Dataset is empty or could not be loaded.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded dataset with {len(data)} rows\")\n",
    "    \n",
    "    # Initialize quiz generator\n",
    "    quiz_generator = QuizGenerator()\n",
    "    \n",
    "    # Generate questions\n",
    "    print(\"Generating questions...\")\n",
    "    questions = create_quiz_from_data(data, quiz_generator)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for idx, question in enumerate(questions, start=1):\n",
    "        print(f\"\\nQuestion {idx}:\")\n",
    "        print(question)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa58f71-71c9-4b33-90d1-ac15b7874333",
   "metadata": {},
   "outputs": [],
   "source": [
    "working code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a4ce60-0ef2-4193-b325-454a3dcaaf22",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TFBaseModelOutput,\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     TFSeq2SeqSequenceClassifierOutput,\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline, AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load summarization model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummarization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/bart-large-cnn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msummarize_content\u001b[39m(content):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Summarizes long chapter content to fit within the model's limit.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:264\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[1;32m--> 264\u001b[0m     _class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m         classes\u001b[38;5;241m.\u001b[39mappend(_class)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1806\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1808\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bart.modeling_tf_bart because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_content(content):\n",
    "    \"\"\"Summarizes long chapter content to fit within the model's limit.\"\"\"\n",
    "    try:\n",
    "        content = str(content).strip()\n",
    "        if not content:\n",
    "            return \"\"\n",
    "            \n",
    "        token_limit = 512  # Lower limit for better question relevance\n",
    "        if len(content.split()) > token_limit:\n",
    "            summary = summarizer(content, max_length=250, min_length=100, do_sample=False)[0]['summary_text']\n",
    "            return summary\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {e}\")\n",
    "        return content\n",
    "\n",
    "class QuizGenerator:\n",
    "    def __init__(self, model_name=\"mistralai/Mistral-7B-Instruct\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_question_from_content(self, content):\n",
    "        \"\"\"Generates a structured multiple-choice question based on chapter content.\"\"\"\n",
    "        try:\n",
    "            if not content:\n",
    "                return \"No valid content to generate a question.\"\n",
    "            \n",
    "            summarized_content = summarize_content(content)\n",
    "            if not summarized_content:\n",
    "                return \"Failed to process content.\"\n",
    "\n",
    "            prompt = (\n",
    "                \"You are a quiz generator AI. Given the following educational content, create a multiple-choice question \"\n",
    "                \"with 4 answer options, clearly marking the correct answer as '[CORRECT]'. Ensure the question is educational, \"\n",
    "                \"clear, and follows a standard MCQ format.\\n\\n\"\n",
    "                f\"Content: {summarized_content}\\n\\n\"\n",
    "                \"Question:\\n\"\n",
    "            )\n",
    "\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(**inputs, max_length=256, temperature=0.7, top_p=0.9)\n",
    "\n",
    "            question = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return question if question else \"Failed to generate question.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error in question generation: {e}\")\n",
    "            return \"Error generating question.\"\n",
    "\n",
    "def load_dataset(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads dataset from an Excel file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath)\n",
    "        if \"Chapter_content\" not in df.columns:\n",
    "            print(\"Warning: 'Chapter_content' column not found in dataset\")\n",
    "            return pd.DataFrame()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_quiz_from_data(data: pd.DataFrame, quiz_generator: QuizGenerator):\n",
    "    \"\"\"Creates quiz questions from the dataset.\"\"\"\n",
    "    questions = []\n",
    "    \n",
    "    for idx, row in data.iterrows():\n",
    "        print(f\"Generating question {idx + 1}/{len(data)}...\")\n",
    "        content = str(row.get(\"Chapter_content\", \"\")).strip()\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        question = quiz_generator.generate_question_from_content(content)\n",
    "        if question and \"Failed to generate\" not in question:\n",
    "            questions.append(question)\n",
    "        \n",
    "        if len(questions) >= 5:  # Limit number of questions\n",
    "            break\n",
    "            \n",
    "    return questions\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    dataset_path = \"dataset_syllabus.xlsx\"\n",
    "    data = load_dataset(dataset_path)\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"Dataset is empty or could not be loaded.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Loaded dataset with {len(data)} rows\")\n",
    "    \n",
    "    quiz_generator = QuizGenerator()\n",
    "    print(\"Generating questions...\")\n",
    "    questions = create_quiz_from_data(data, quiz_generator)\n",
    "    \n",
    "    print(\"\\nGenerated Questions:\")\n",
    "    for idx, question in enumerate(questions, start=1):\n",
    "        print(f\"\\nQuestion {idx}:\")\n",
    "        print(question)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd67c958-72ce-40f6-9452-84ecf1b552d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4062 > 1024). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\Shalini\\my_design_venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 4062, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 107\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m data \u001b[38;5;241m=\u001b[39m load_dataset(dataset_path)\n\u001b[0;32m    106\u001b[0m quiz_generator \u001b[38;5;241m=\u001b[39m QuizGenerator()\n\u001b[1;32m--> 107\u001b[0m quiz \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_quiz_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiz_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     question \u001b[38;5;241m=\u001b[39m quiz\u001b[38;5;241m.\u001b[39mget_current_question()\n",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m, in \u001b[0;36mcreate_quiz_from_data\u001b[1;34m(data, quiz_generator)\u001b[0m\n\u001b[0;32m     90\u001b[0m num_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Adjustable number of questions\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_questions):\n\u001b[1;32m---> 92\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[43mquiz_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_question_from_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChapter_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     correct_answer \u001b[38;5;241m=\u001b[39m quiz_generator\u001b[38;5;241m.\u001b[39mgenerate_question_from_content(Chapter_content)\n\u001b[0;32m     94\u001b[0m     options \u001b[38;5;241m=\u001b[39m quiz_generator\u001b[38;5;241m.\u001b[39mgenerate_options(correct_answer, Chapter_content)\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mQuizGenerator.generate_question_from_content\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m     16\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate a multiple-choice question based on the following chapter content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m question\n",
      "File \u001b[1;32m~\\my_design_venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\my_design_venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2108\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_supports_num_logits_to_keep() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   2106\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_logits_to_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2108\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# 7. Prepare the cache.\u001b[39;00m\n\u001b[0;32m   2111\u001b[0m \u001b[38;5;66;03m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;66;03m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[0;32m   2113\u001b[0m \u001b[38;5;66;03m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m \u001b[38;5;66;03m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[0;32m   2115\u001b[0m cache_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_params\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\my_design_venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1411\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[1;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m   1410\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1415\u001b[0m     )\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1421\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Input length of input_ids is 4062, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class QuizGenerator:\n",
    "    def __init__(self, model_name: str = \"gpt2\"):\n",
    "        \"\"\"Initialize the quiz generator with a pre-trained model.\"\"\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_question_from_content(self, content: str) -> str:\n",
    "        \"\"\"Generate a multiple-choice question from the given chapter content.\"\"\"\n",
    "        prompt = f\"Generate a multiple-choice question based on the following chapter content: {content}\\nQuestion:\"\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        question = self.tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "        return question\n",
    "\n",
    "    def generate_options(self, correct_answer: str, content: str) -> List[str]:\n",
    "        \"\"\"Generate plausible options including the correct answer based on chapter content.\"\"\"\n",
    "        options = [correct_answer]\n",
    "        prompt = f\"Given the chapter content: {content}, and the correct answer: {correct_answer}, generate an alternative answer:\"\n",
    "        \n",
    "        while len(options) < 4:\n",
    "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=50,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=2,\n",
    "                temperature=0.8\n",
    "            )\n",
    "            new_option = self.tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "            if new_option and new_option not in options:\n",
    "                options.append(new_option)\n",
    "        \n",
    "        random.shuffle(options)\n",
    "        return options\n",
    "\n",
    "class Quiz:\n",
    "    def __init__(self, chapter_content: str):\n",
    "        \"\"\"Initialize a quiz with chapter content.\"\"\"\n",
    "        self.chapter_content = chapter_content\n",
    "        self.questions = []\n",
    "        self.current_question = 0\n",
    "        self.score = 0\n",
    "\n",
    "    def add_question(self, question: str, options: List[str], correct_answer: str):\n",
    "        \"\"\"Add a question to the quiz.\"\"\"\n",
    "        self.questions.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"correct_answer\": correct_answer\n",
    "        })\n",
    "\n",
    "    def check_answer(self, answer: str) -> bool:\n",
    "        \"\"\"Check if the provided answer is correct.\"\"\"\n",
    "        if self.current_question >= len(self.questions):\n",
    "            return False\n",
    "        is_correct = answer == self.questions[self.current_question][\"correct_answer\"]\n",
    "        if is_correct:\n",
    "            self.score += 1\n",
    "        self.current_question += 1\n",
    "        return is_correct\n",
    "\n",
    "    def get_current_question(self) -> Dict:\n",
    "        \"\"\"Get the current question.\"\"\"\n",
    "        if self.current_question >= len(self.questions):\n",
    "            return None\n",
    "        return self.questions[self.current_question]\n",
    "\n",
    "    def get_score(self) -> Tuple[int, int]:\n",
    "        \"\"\"Get the current score and total questions.\"\"\"\n",
    "        return self.score, len(self.questions)\n",
    "\n",
    "def create_quiz_from_data(data: pd.DataFrame, quiz_generator: QuizGenerator) -> Quiz:\n",
    "    \"\"\"Create a quiz from the dataset using only chapter content.\"\"\"\n",
    "    row = data.sample(1).iloc[0]\n",
    "    Chapter_content = row['Chapter_content']\n",
    "    quiz = Quiz(Chapter_content)\n",
    "    num_questions = 5  # Adjustable number of questions\n",
    "    for _ in range(num_questions):\n",
    "        question = quiz_generator.generate_question_from_content(Chapter_content)\n",
    "        correct_answer = quiz_generator.generate_question_from_content(Chapter_content)\n",
    "        options = quiz_generator.generate_options(correct_answer, Chapter_content)\n",
    "        quiz.add_question(question, options, correct_answer)\n",
    "    return quiz\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(filepath: str) -> pd.DataFrame:\n",
    "    return pd.read_excel(filepath)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    dataset_path = \"dataset_syllabus.xlsx\"\n",
    "    data = load_dataset(dataset_path)\n",
    "    quiz_generator = QuizGenerator()\n",
    "    quiz = create_quiz_from_data(data, quiz_generator)\n",
    "    \n",
    "    while True:\n",
    "        question = quiz.get_current_question()\n",
    "        if not question:\n",
    "            break\n",
    "        print(\"\\nQuestion:\", question['question'])\n",
    "        for i, option in enumerate(question['options']):\n",
    "            print(f\"{chr(65 + i)}. {option}\")\n",
    "        answer = input(\"\\nYour answer (A/B/C/D): \").upper()\n",
    "        if answer in ['A', 'B', 'C', 'D']:\n",
    "            selected_answer = question['options'][ord(answer) - ord('A')]\n",
    "            is_correct = quiz.check_answer(selected_answer)\n",
    "            print(\"Correct!\" if is_correct else \"Incorrect!\")\n",
    "    \n",
    "    score, total = quiz.get_score()\n",
    "    print(f\"\\nFinal Score: {score}/{total}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a6f019f-4385-4f66-b482-cbb16eb67b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " def generate_options(self, correct_answer: str, content: str) -> List[str]:\n",
    "        \"\"\"Generate plausible options including the correct answer.\"\"\"\n",
    "        options = [correct_answer]\n",
    "        prompt = f\"Given the topic: {content}, and the correct answer: {correct_answer}, generate an alternative answer:\"\n",
    "        \n",
    "        while len(options) < 4:\n",
    "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=50,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=2,\n",
    "                temperature=0.8\n",
    "            )\n",
    "            new_option = self.tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "            if new_option and new_option not in options:\n",
    "                options.append(new_option)\n",
    "        \n",
    "        random.shuffle(options)\n",
    "        return options\n",
    "\n",
    "class Quiz:\n",
    "    def __init__(self, board: str, grade: int, subject: str, chapter: str):\n",
    "        \"\"\"Initialize a quiz with metadata.\"\"\"\n",
    "        self.board = board\n",
    "        self.grade = grade\n",
    "        self.subject = subject\n",
    "        self.chapter = chapter\n",
    "        self.questions = []\n",
    "        self.current_question = 0\n",
    "        self.score = 0\n",
    "\n",
    "    def add_question(self, question: str, options: List[str], correct_answer: str):\n",
    "        \"\"\"Add a question to the quiz.\"\"\"\n",
    "        self.questions.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"correct_answer\": correct_answer\n",
    "        })\n",
    "\n",
    "    def check_answer(self, answer: str) -> bool:\n",
    "        \"\"\"Check if the provided answer is correct.\"\"\"\n",
    "        if self.current_question >= len(self.questions):\n",
    "            return False\n",
    "        is_correct = answer == self.questions[self.current_question][\"correct_answer\"]\n",
    "        if is_correct:\n",
    "            self.score += 1\n",
    "        self.current_question += 1\n",
    "        return is_correct\n",
    "\n",
    "    def get_current_question(self) -> Dict:\n",
    "        \"\"\"Get the current question.\"\"\"\n",
    "        if self.current_question >= len(self.questions):\n",
    "            return None\n",
    "        return self.questions[self.current_question]\n",
    "\n",
    "    def get_score(self) -> Tuple[int, int]:\n",
    "        \"\"\"Get the current score and total questions.\"\"\"\n",
    "        return self.score, len(self.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d2f93be-c574-4c6b-89ad-37c0ece49436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quiz_from_data(data: pd.DataFrame, quiz_generator: QuizGenerator) -> Quiz:\n",
    "    \"\"\"Create a quiz from the dataset.\"\"\"\n",
    "    row = data.sample(1).iloc[0]\n",
    "    quiz = Quiz(\n",
    "        board=row['Board'],\n",
    "        grade=row['Grade'],\n",
    "        subject=row['Subject'],\n",
    "        chapter=row['Chapter']\n",
    "    )\n",
    "    content = row['chapter_content']\n",
    "    num_questions = 5  # Adjustable number of questions\n",
    "    for _ in range(num_questions):\n",
    "        question = quiz_generator.generate_question_from_content(content)\n",
    "        correct_answer = quiz_generator.generate_question_from_content(content)\n",
    "        options = quiz_generator.generate_options(correct_answer, content)\n",
    "        quiz.add_question(question, options, correct_answer)\n",
    "    return quiz\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(filepath: str) -> pd.DataFrame:\n",
    "    return pd.read_excel(filepath)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    dataset_path = \"dataset_syllabus.xlsx\"\n",
    "    data = load_dataset(dataset_path)\n",
    "    quiz_generator = QuizGenerator()\n",
    "    quiz = create_quiz_from_data(data, quiz_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c571ef9a-d357-4a4a-944a-0e4cde7af397",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quiz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[43mquiz\u001b[49m\u001b[38;5;241m.\u001b[39mget_current_question()\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m question:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quiz' is not defined"
     ]
    }
   ],
   "source": [
    "    while True:\n",
    "        question = quiz.get_current_question()\n",
    "        if not question:\n",
    "            break\n",
    "        print(\"\\nQuestion:\", question['question'])\n",
    "        for i, option in enumerate(question['options']):\n",
    "            print(f\"{chr(65 + i)}. {option}\")\n",
    "        answer = input(\"\\nYour answer (A/B/C/D): \").upper()\n",
    "        if answer in ['A', 'B', 'C', 'D']:\n",
    "            selected_answer = question['options'][ord(answer) - ord('A')]\n",
    "            is_correct = quiz.check_answer(selected_answer)\n",
    "            print(\"Correct!\" if is_correct else \"Incorrect!\")\n",
    "    \n",
    "    score, total = quiz.get_score()\n",
    "    print(f\"\\nFinal Score: {score}/{total}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2d474a-a571-4f5a-9df3-5c329c5ef174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quiz:\n",
    "    def __init__(self, board: str, grade: int, subject: str, chapter: str):\n",
    "        \"\"\"Initialize a quiz with metadata.\"\"\"\n",
    "        self.board = board\n",
    "        self.grade = grade\n",
    "        self.subject = subject\n",
    "        self.chapter = chapter\n",
    "        self.questions = []\n",
    "        self.current_question = 0\n",
    "        self.score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41b4166e-af50-4370-b9c0-e1f34d54dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_question(self, question: str, options: List[str], correct_answer: str):\n",
    "        \"\"\"Add a question to the quiz.\"\"\"\n",
    "        self.questions.append({\n",
    "            \"question\": question,\n",
    "            \"options\": options,\n",
    "            \"correct_answer\": correct_answer\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef03620a-025b-44aa-ae5a-61f3ad86b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(self, answer: str) -> bool:\n",
    "        \"\"\"Check if the provided answer is correct.\"\"\"\n",
    "        if self.current_question >= len(self.questions):\n",
    "            return False\n",
    "        \n",
    "        is_correct = answer == self.questions[self.current_question][\"correct_answer\"]\n",
    "        if is_correct:\n",
    "            self.score += 1\n",
    "        self.current_question += 1\n",
    "        return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "799ea2ed-54da-41d7-9d9f-136536a76872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_question(self) -> Dict:\n",
    "        \"\"\"Get the current question.\"\"\"\n",
    "        if self.current_question >= len(self.questions):\n",
    "            return None\n",
    "        return self.questions[self.current_question]\n",
    "\n",
    "def get_score(self) -> Tuple[int, int]:\n",
    "        \"\"\"Get the current score and total questions.\"\"\"\n",
    "        return self.score, len(self.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a864f17-d44d-4f23-aa8f-c66c2f917957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quiz_from_data(data: pd.DataFrame, quiz_generator: QuizGenerator) -> Quiz:\n",
    "    \"\"\"Create a quiz from the provided dataset.\"\"\"\n",
    "    # Sample implementation - you'll need to adapt this to your actual data structure\n",
    "    row = data.sample(1).iloc[0]\n",
    "    \n",
    "    quiz = Quiz(\n",
    "        board=row['Board'],\n",
    "        grade=row['Grade'],\n",
    "        subject=row['Subject'],\n",
    "        chapter=row['Chapter_title']\n",
    "    )\n",
    "    \n",
    "    # Generate questions from chapter content\n",
    "    content = row['Chapter_content']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acb46c48-8f55-423b-b4ac-209f5aedab68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quiz_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m num_questions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# You can adjust this number\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_questions):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;66;03m# Generate question and correct answer\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m         question \u001b[38;5;241m=\u001b[39m \u001b[43mquiz_generator\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate_question_from_content(content)\n\u001b[0;32m      5\u001b[0m         correct_answer \u001b[38;5;241m=\u001b[39m quiz_generator\u001b[38;5;241m.\u001b[39mgenerate_question_from_content(content)  \u001b[38;5;66;03m# This is simplified\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Generate options\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quiz_generator' is not defined"
     ]
    }
   ],
   "source": [
    "num_questions = 20  # You can adjust this number\n",
    "for _ in range(num_questions):\n",
    "        # Generate question and correct answer\n",
    "        question = quiz_generator.generate_question_from_content(content)\n",
    "        correct_answer = quiz_generator.generate_question_from_content(content)  # This is simplified\n",
    "        \n",
    "        # Generate options\n",
    "        options = quiz_generator.generate_options(correct_answer, content)\n",
    "        \n",
    "        # Add question to quiz\n",
    "        quiz.add_question(question, options, correct_answer)\n",
    "    \n",
    "return quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9324a5a-fc66-4e32-bb87-4104326dfd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
